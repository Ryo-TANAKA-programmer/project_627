---
  title: "STAT-627_PRJ"
format: pdf
editor: visual
---

# Hey Bright!! I got your message!

# Please **git pull** first before you edit the file!

```{r}
library(dplyr)
library(tidyverse)
library(leaps)
library(rms)
library(MASS)
library(glmnet)
library(caret)
library(boot)
library(car)
library(olsrr)
library(tree)
library(FNN)
library(class)
library(pROC)
library(ggplot2)
library(kknn)
```

## Data preparation

```{r}

setwd(getwd()) # setting working directory for users

diabetes_data <- read.csv("Dataset_of_Diabetes.csv")

# To make Tree, I converted BMI into factor-based variable
# 1: Underweight, 2: Normal, 3: Overweight 4: Obese
diabetes_data$BMI_Category <- cut(diabetes_data$BMI,
                                  breaks = c(-Inf, 18.5, 24.9, 29.9, Inf),
                                  labels = c(0,1,2,3))

# diabetes_data$BMI_Category <- cut(diabetes_data$BMI,
#                                   breaks = c(-Inf, 18.5, 24.9, 29.9, Inf),
#                                   labels = c("Underweight", "Normal", "Overweight", "Obese"))
str(diabetes_data)
```

## Data Cleaning

```{r}
## checking consistency of the levels in "Gender", "CLASS","BMI_Category"
unique_values <- lapply(diabetes_data[c("Gender", "CLASS","BMI_Category")], unique)
unique_values

# ## Fixing f = "F" and "N " = "N","Y "= "Y"
# diabetes <- diabetes_data %>%
#   mutate(across(c(3, 14), ~ recode(., "f" = "F", "N " = "N", "Y " = "Y")))

diabetes <- diabetes_data

unique_values1 <- lapply(diabetes[c("Gender", "CLASS","BMI_Category")], unique)

# checking incomplete obs
diabetes <- diabetes %>%
  mutate(missing_count_SPS = rowSums(is.na(.[1:15])))

# Removing ID and Patient Number and missing_count since we don't need them
diabetes <- diabetes[-c(1,2,16)]

# make CLASS to 0 and 1
diabetes <- diabetes %>%
  mutate(CLASS_BINARY = if_else(CLASS == "Y" | CLASS == "P", 1, 0))

# Converting CLASS_BINARY
diabetes$CLASS_BINARY <- as.factor(diabetes$CLASS_BINARY)
```

## Summary of the entire Dataset

```{r}
summary(diabetes)
```

```{r}
# summary of BMI
BMI_summary <- diabetes %>%
  group_by(Gender, BMI_Category) %>%
  summarise(Count = n(), .groups = 'drop')
BMI_summary
```

```{r}
# 1: Underweight, 2: Normal, 3: Overweight 4: Obese

gg_base <- ggplot(data = diabetes) 

gg_base + geom_bar(mapping = aes(x = Gender,
                                 fill = BMI_Category),
                   position = "fill") +
  theme_bw() +
  labs(y = "Pencentage",
       fill = "BMI Category")

```

```{r}
gg_base + geom_bar(mapping = aes(x = Gender,
                                 fill = as.factor(CLASS_BINARY)),
                   position = "fill") +
  theme_bw() + 
  labs(y = "Pencentage",
       fill = "Diabetes") 
```

```{r}
par(mfrow = c(5, 2))
diabetes_numeric_df <- diabetes[-c(1,12,13,14)]
for (col in colnames(diabetes_numeric_df)) {
  hist(diabetes_numeric_df[[col]], 
       main = col, 
       xlab = "")
}
```

```{r}
cor(diabetes_numeric_df)
```

-   Cr(Creatinine ratio) and Urea are categorized as highly correlated, therefore, we may want to deal with these variables. It is not surprising to see LDL and Cholesterole are also moderaly correlated.

```{r}
pairs(diabetes_numeric_df)
```

```{r}
diabetes$Gender <- as.factor(diabetes$Gender)
is.factor(diabetes$Gender)
diabetes <- diabetes %>%
  mutate(Gender = if_else(Gender == "F",0,1))
diabetes$Gender <- as.factor(diabetes$Gender)
```

## Tree

```{r}
diabetes <- diabetes %>% 
  mutate(Gender = if_else(diabetes$Gender == "M",1,0))
diabetes$Gender <- as.factor(diabetes$Gender)

# removed BMI and CLASS since we now have categorical variables 
diabetes_tree <- diabetes[-c(12,13)]

# The following command will randomly select 60% of the row numbers in the data set to represent the training data
training <- sample(1:nrow(diabetes_tree), 
                   0.6*nrow(diabetes_tree))

# finds the column number of the dependent variable by its name.
ycol <- match('CLASS_BINARY',
              colnames(diabetes_tree))

# separate the training data into two variables, one with CLASS_BINARY removed, and one containing only CLASS_BINARY
set.seed(1234)
DIA_training <- diabetes_tree[training,-ycol]

# For this purpose, converting diabetes$CLASS_BINARY into double
# diabetes$CLASS_BINARY <- as.numeric(diabetes$CLASS_BINARY)
diabetes_tree$CLASS_BINARY <- as.numeric(diabetes$CLASS_BINARY)

diabetes_tree <- diabetes_tree %>% 
  mutate(CLASS_BINARY = if_else(diabetes_tree$CLASS_BINARY == 1,
                                0,
                                1))
typeof(diabetes$CLASS_BINARY)

DIA_training_results <- diabetes_tree[training,
                                      ycol] > 0.5

# Do the same for the remaining 40% of the data
# Here, again, the second command stores the results as binary variables
DIA_test <- diabetes_tree[-training,-ycol]

DIA_test_results <- diabetes_tree[-training,
                             ycol] > 0.5
```

### Creating a model for classification tree

```{r}
# Put this back to binary
diabetes$CLASS_BINARY <- as.factor(diabetes$CLASS_BINARY)

DIA_tree <- tree(CLASS_BINARY ~ Gender + HbA1c + Chol + TG + BMI,
                 data = diabetes_tree[training,])

plot(DIA_tree)
text(DIA_tree)
```

```{r}
# Obtains the proportions of 1's from the training set, in the tree endpoint for each data point in the test set

DIA_tree_proportions <- predict(DIA_tree,
                               diabetes_tree[-training,])

# Rounds each proportion to 0 or 1, obtaining the binary classifications
DIA_tree_classifications <- round(x = DIA_tree_proportions,
                                 digits = 0)

# Computes the proportion of classifications on the test set that were correct
sum(DIA_tree_classifications == DIA_test_results) / nrow(diabetes[-training,])

# The following command shows the classification confusion matrix for the test set
table(DIA_tree_classifications, DIA_test_results)

best.mindev <- -1
error.rate <- -1
best.error.rate <- 99999999
for (i in seq(from = 0.0005, 
              to = 0.05, 
              by = 0.0005)) {
  DIA_tree <- tree(CLASS_BINARY ~ Gender + 
                     HbA1c + 
                     Chol + 
                     TG + 
                     BMI,
                   data = diabetes_tree[training,],
                   mindev = i)
  DIA_tree_proportions <- predict(DIA_tree,
                                  diabetes_tree[-training,])
  DIA_tree_classifications <- round(DIA_tree_proportions,
                                    0)
  error.rate <- 1- (sum(DIA_tree_classifications == DIA_test_results) / nrow(diabetes_tree[-training,]))
  if (error.rate < best.error.rate) {
    best.mindev <- i
    best.error.rate <- error.rate
  }
}

print(paste("The optimal value of mindev is",best.mindev,"with an accuracy rate",1 -best.error.rate))

# The following commands re-create and plot the optimal tree
DIA_BEST_TREE <- tree(CLASS_BINARY ~ Gender + 
                     HbA1c + 
                     Chol + 
                     TG + 
                     BMI, 
                      data = diabetes_tree[training,], 
                      mindev = best.mindev)
plot(DIA_BEST_TREE)
text(DIA_BEST_TREE, cex=0.5)
```

## Ryo: KNN

```{r}
# removed BMI and CLASS since we now have categorical variables 
diabetes_knn <- diabetes[-c(11,12)]
diabetes_knn$Gender <- as.numeric(diabetes_knn$Gender)
diabetes_knn$BMI_Category <- as.numeric(diabetes_knn$BMI_Category)
diabetes_knn$CLASS_BINARY <- as.numeric(diabetes_knn$CLASS_BINARY)

# no 1 confirmed: diabetes_knn$BMI_Category[diabetes_knn$BMI_Category == 1] <- 0
diabetes_knn$BMI_Category[diabetes_knn$BMI_Category == 2] <- 1
diabetes_knn$BMI_Category[diabetes_knn$BMI_Category == 3] <- 2
diabetes_knn$BMI_Category[diabetes_knn$BMI_Category == 4] <- 3

# Re-labelling here for CLASS_BINARY
diabetes_knn$CLASS_BINARY[diabetes_knn$CLASS_BINARY == 1] <- 0
diabetes_knn$CLASS_BINARY[diabetes_knn$CLASS_BINARY == 2] <- 1


# The following command will randomly select 60% of the row numbers in the data set to represent the training data
training <- sample(1:nrow(diabetes_knn), 
                   0.5*nrow(diabetes_knn))

# finds the column number of the dependent variable by its name.
ycol <- match('CLASS_BINARY',
              colnames(diabetes_knn))

# separate the training data into two variables, one with CLASS_BINARY removed, and one containing only CLASS_BINARY
set.seed(1234)
DIA_KNN_training <- diabetes_knn[training,-ycol]
DIA_KNN_training_results <- diabetes_knn[training,ycol] > 0.5

# Do the same for the remaining 40% of the data
# Here, again, the second command stores the results as binary variables
DIA_KNN_test <- diabetes_knn[-training,-ycol]

DIA_KNN_test_results <- diabetes_knn[-training,ycol] > 0.5


# The following command runs the k-nearest neighbors model with k=5
DIA_knn <- knn(DIA_KNN_training, 
               DIA_KNN_test, 
               DIA_KNN_training_results,
               k = 8)

# The following command computes the proportion of classifications on the test set that were correct
1 - sum(DIA_knn == DIA_KNN_test_results) / length(DIA_KNN_test_results)

# The following command shows the classification confusion matrix for the test set.
table(DIA_knn, DIA_KNN_test_results)

best.k <- -1
error.rate <- -1
best.error.rate <- 99999
for (i in 1:100) {
  set.seed(1234)
  
  DIA_len_knn <- knn(DIA_KNN_training, DIA_KNN_test, DIA_KNN_training_results, k = i)
  
  error.rate <- 1 - (sum(DIA_len_knn == DIA_KNN_test_results) / length(DIA_KNN_test_results))
  if (error.rate < best.error.rate) {
    best.k <- i
    best.error.rate <- error.rate
  }
}

print(paste("The optimal value of k is",best.k,"with an overall error rate of",best.error.rate))
```

## Ryo: LOOCV

```{r}
diabetes_LOOCV <- diabetes[-c(11,12)]

```

## Ryo and Beau is working on LOOCV using Logistic Regression

```{r}
# probably works?
full_reg_BINARY_LOOCV <- glm(CLASS_BINARY ~ Gender + AGE + Urea + Cr + HbA1c + Chol + TG + 
                         HDL + LDL + VLDL + BMI_Category,
                       family = "binomial",
                       data = diabetes)   # initial model

cv_loocv <- cv.glm(diabetes, full_reg_BINARY_LOOCV, K = nrow(diabetes))

loocv_error <- cv_loocv$delta[1]

# Print the LOOCV prediction error
print(paste("LOOCV Prediction Error:", loocv_error))
```

## Beau: Optimal threshold for logistic regression

```{r}
# Tuning model to get Optimal threshold for logistic regression

set.seed(1234) 
training_pct <- 0.5
Z <- sample(nrow(diabetes), floor(training_pct * nrow(diabetes)))

training_data <- diabetes[Z, ]

test_data <- diabetes[-Z, ]

full_reg_BINARY_LOOCV <- glm(CLASS_BINARY ~ Gender + AGE + Urea + Cr + HbA1c + Chol + TG + 
                         HDL + LDL + VLDL + BMI_Category,
                       family = "binomial",
                       data = training_data)   # initial model

# Predict probabilities on the test data
Prob <- predict(full_reg_BINARY_LOOCV, newdata = test_data, type = "response")

threshold <- seq(0, 1, 0.01)

# Initialize vectors to store rates for various thresholds
TPR <- FPR <- err.rate <- numeric(length(threshold))

# Loop over thresholds to calculate TPR, FPR, and error rate
for (i in seq_along(threshold)) {
    # Classify as 'yes' if probability >= threshold, else 'no'
    # Assuming 'yes' corresponds to CLASS_BINARY == 1
    Yhat <- ifelse(Prob >= threshold[i], 1, 0)
    
    # Actual binary outcomes from the test data
    Actual <- test_data$CLASS_BINARY
    
    # Calculate error rate, True Positive Rate (TPR), and False Positive Rate (FPR)
    err.rate[i] <- mean(Yhat != Actual)
    TPR[i] <- sum(Yhat == 1 & Actual == 1) / sum(Actual == 1)
    FPR[i] <- sum(Yhat == 1 & Actual == 0) / sum(Actual == 0)
}

# Find the threshold with the minimum error rate
optimal_index <- which.min(err.rate)
optimal_threshold <- threshold[optimal_index]

print(paste("Optimal threshold:", optimal_threshold))
```

```{r}
#plot of Optimal threshold: (the prediction of accuracy rate is so high????)
ggplot(tibble(threshold, err.rate),
       aes(threshold, err.rate)) + 
  geom_point()

```

```{r}
Yhat <- ifelse(Prob >= threshold[which.min(err.rate)], "1", "0")

Actual <- test_data$CLASS_BINARY

accuracy <- sum(Yhat == Actual) / length(Actual)
print(paste("Accuracy rate after applying the optimal threshold:", accuracy))
table(Yhat, test_data$CLASS_BINARY)
text(DIA_BEST_TREE, cex = 0.8)
```

## Sheila: KNN

```{r}
set.seed(1234) 

n <- nrow(diabetes)
training_pct <- 0.75 

Z <- sample(seq_len(n), size = floor(n * training_pct))

train_data <- diabetes[Z, ]
test_data <- diabetes[-Z, ]

X_train <- model.matrix(CLASS_BINARY ~ Gender + AGE + Urea + Cr + HbA1c + Chol + TG +HDL + LDL + VLDL + BMI + BMI_Category, data = train_data)[,-1]
Y_train <- train_data$CLASS_BINARY
X_test <- model.matrix(CLASS_BINARY ~ Gender + AGE + Urea + Cr + HbA1c + Chol + TG + HDL + LDL + VLDL + BMI + BMI_Category, data = test_data)[,-1]
Y_test <- test_data$CLASS_BINARY
```

```{r}
library(ggplot2)
library(dplyr)

err_class <- numeric(100)
tpr <- numeric(100)
fpr <- numeric(100)

for (k in 1:100) {
  Yhat <- knn(X_train, X_test, Y_train, k = k)
  err_class[k] <- mean(Yhat != Y_test)
  # Adjust the following calculations as needed for your specific case
  tpr[k] <- sum(Yhat == 1 & Y_test == 1) / sum(Y_test == 1)
  fpr[k] <- sum(Yhat == 1 & Y_test == 0) / sum(Y_test == 0)
}

ggplot(tibble(err_class, k = 1:100), aes(x = k, y = err_class)) +
  geom_line()
```

```{r}
which.min(err_class) # gives the k
```

```{r}
err_class[which.min(err_class)] # Probability of a Mis-classification 
```

```{r}
1 - err_class[which.min(err_class)] # Probability of a Correct Classification
```

```{r}
# calculate the accuracy for the best k
Yhat <-  knn(X_train, X_test, Y_train, k = which.min(err_class)) 
table(Y_test, Yhat)
```

```{r}
# Accuracy
(table(Y_test, Yhat)[1, 1] + table(Y_test, Yhat)[2, 2])/((1-0.75)*nrow(diabetes))
```

```{r}
Yhat <-  knn(X_train,X_test, Y_train, k = 100) 
table(Y_test, Yhat) # the Confusion Matrix
```

```{r}
# Accuracy
(table(Y_test, Yhat)[1, 1] + table(Y_test, Yhat)[2, 2])/(.75*nrow(diabetes))
```

```{r}
Yhat <-  knn(X_train, X_test, Y_train, k = which.min(err_class)) 
table(Y_test, Yhat) # the Confusion Matrix
```

```{r}
ggplot(tibble(tpr, fpr), aes(x = fpr, y = tpr)) +
  geom_line() +
  geom_abline(color = "red", lty = 3) +
  ylim(0, 1) + xlim(0, 1) +
  geom_hline(yintercept = mean(as.numeric(diabetes$CLASS)-1), color = "green", lty = 2)
```

## Sheila: KNN regression

```{r}
formula <- CLASS_BINARY ~ Gender + AGE + Urea + Cr + HbA1c + Chol + TG +HDL + LDL + VLDL + BMI + BMI_Category

fit <- kknn(formula, train_data, test_data, k = 11, distance = 1, kernel = "optimal")

predictions <- fitted(fit)

confusionMatrix(data = predictions, reference = as.factor(test_data$CLASS_BINARY))
```

## Beau: STEP WISE

```{r}
fit.small <- glm(CLASS_BINARY~ 1,
                 family = "binomial",
                data = diabetes)

full_reg_BINARY <- glm(CLASS_BINARY ~ Gender + AGE + Urea + Cr + HbA1c + Chol + TG + 
                         HDL + LDL + VLDL + BMI + BMI_Category,
                       family = "binomial",
                       data = diabetes)   # initial model

fit.step.15p <- step(full_reg_BINARY,
                     scope = list(lower=fit.small,
                                  upper=full_reg_BINARY),
                     direction = "both",
                     test = "F")
summary(fit.step.15p)


# last step has the best AIC which the best model is 
# Gender
# HbA1c
# Chol (Cholesterol)
# TG (Triglycerides)
# BMI (Body Mass Index)
# BMI_Category                             with response variable CLASS_BINARY
```

## CI

```{r}
model_matrix <- model.matrix(full_reg, data = diabetes)[,-1]
standardized_model_matrix <- scale(model_matrix)
svd_results <- svd(standardized_model_matrix)

eigenvalues <- svd_results$d^2
condition_index <- sqrt(max(eigenvalues) / eigenvalues)


column_names <- colnames(model_matrix)

ci_with_names <- data.frame(Variable = column_names, Condition_Index = condition_index)

print(ci_with_names)

# Low Condition Index (< 30) all of our predictors are fall within 30, unlikely to have multicollinearity.
```

## VIF

```{r}
vif_values <- vif(full_reg)

vif_values

# BMI category has multicollinearity issue, but i think its fine cause they are all from BMI.
# For the predictors with VIF values below 5, they are generally not concern of multicollinearity.


```

## Ridge and Lasso

```{r}
library(caret)
set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(diabetes), replace=TRUE, prob=c(0.5, 0.5))
train_data <- diabetes[sample, ]
test_data <- diabetes[!sample, ]

# Create Design Matrices
x_train <- model.matrix(CLASS_BINARY ~ Gender + AGE + Urea + Cr + HbA1c + Chol + TG + 
                          HDL + LDL + VLDL + BMI + BMI_Category, data = train_data)[,-1] 
y_train <- train_data$CLASS_BINARY

x_test <- model.matrix(CLASS_BINARY ~ Gender + AGE + Urea + Cr + HbA1c + Chol + TG + 
                         HDL + LDL + VLDL + BMI + BMI_Category, data = test_data)[,-1] 
y_test <- test_data$CLASS_BINARY

# Ridge Regression 

cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0, family = "binomial")
opt_lambda_ridge <- cv_ridge$lambda.min
ridge_model <- glmnet(x_train, y_train, alpha = 0, lambda = opt_lambda_ridge, family = "binomial")


# Lasso Regression 

cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
opt_lambda_lasso <- cv_lasso$lambda.min
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = opt_lambda_lasso, family = "binomial")

```

```{r}
# Evaluate Models


# Predictions on the testing set
predictions_ridge <- predict(ridge_model, s = opt_lambda_ridge, newx = x_test, type = "response")
predictions_lasso <- predict(lasso_model, s = opt_lambda_lasso, newx = x_test, type = "response")

# Convert predictions to binary class based on a threshold 0.5
predictions_ridge_binary <- ifelse(predictions_ridge > 0.5, 1, 0)
predictions_lasso_binary <- ifelse(predictions_lasso > 0.5, 1, 0)

# Calculate Accuracy 
accuracy_ridge <- mean(predictions_ridge_binary == y_test)
accuracy_lasso <- mean(predictions_lasso_binary == y_test)

# Print Accuracy for comparison
print(paste("Accuracy for Ridge Regression:", accuracy_ridge)) # 88% Accuracy rate
print(paste("Accuracy for Lasso Regression:", accuracy_lasso)) # 11% Accuracy rate
```

# Bright Work

-   logistic regression

```{r}

full_reg_BINARY1 <- glm(CLASS_BINARY ~ Gender + AGE + Urea + Cr + HbA1c + Chol + TG + 
                         HDL + LDL + VLDL + BMI ,
                       family = "binomial",
                       data = diabetes)   # initial model
summary(full_reg_BINARY1)

vif_values <- vif(full_reg_BINARY1)

vif_values

reg_full_model <- glm(CLASS_BINARY ~ Gender + AGE + Urea + Cr + HbA1c + Chol + TG + 
                         HDL + LDL + VLDL + BMI ,
                       family = "binomial",
                       data = diabetes)
reg_null_model <- glm(CLASS_BINARY ~ 1,family = "binomial", data = diabetes)

fit.step.11p <- step(full_reg_BINARY1,
                     scope = list(lower=reg_null_model,
                                  upper=full_reg_BINARY1),
                     direction = "both",
test = "F")
summary(fit.step.11p)
```

```{r}
library(GGally)
ggpairs(diabetes[,-c(2,3,4,8,9,10,12,13)])
```

```{r}
ggplot(diabetes, aes(BMI_Category,HbA1c, color = CLASS_BINARY)) +
  geom_jitter(size = .5)

ggplot(diabetes, aes(BMI_Category,Chol, color = CLASS_BINARY)) +
  geom_jitter(size = .5)

ggplot(diabetes, aes(BMI_Category,TG  , color = CLASS_BINARY)) +
  geom_jitter(size = .5)

ggplot(diabetes, aes(BMI_Category,Gender  , color = CLASS_BINARY)) +
  geom_jitter(size = .5)


```

## LDA

```{r}
set.seed(123)
n <- nrow(diabetes)
Z <- sample(n, n/2)
# 1st slipt
Data_training = diabetes[Z, ]
Data_testing = diabetes[-Z, ]
# 2nd split
Data_training2 = diabetes[Z, ]
Data_testing2 = diabetes[-Z, ]

# Using first set of the split
lda_out <- lda(CLASS_BINARY ~ Gender + HbA1c + Chol + TG + BMI,data = Data_training) # don't need CV = TRUE

Predicted.CLASS_lda  <-  predict(lda_out, data.frame(Data_testing))$class

table(Data_testing$CLASS_BINARY, Predicted.CLASS_lda)

# Prediction Correct Classification Rate
round(mean(Data_testing$CLASS_BINARY == Predicted.CLASS_lda), 3)

round(mean(Predicted.CLASS_lda == "1"), 3)

round(mean(Data_testing$CLASS_BINARY == "1"), 3)

# Using first set of the split
lda_out2 <- lda(CLASS_BINARY ~ Gender + HbA1c + Chol + TG + BMI,data = Data_training2) # don't need CV = TRUE

Predicted.CLASS_lda2  <-  predict(lda_out2, data.frame(Data_testing2))$class

table(Data_testing2$CLASS_BINARY, Predicted.CLASS_lda2)

# Prediction Correct Classification Rate
round(mean(Data_testing2$CLASS_BINARY == Predicted.CLASS_lda2), 3)

round(mean(Predicted.CLASS_lda2 == "1"), 3)

round(mean(Data_testing2$CLASS_BINARY == "1"), 3)




```

QDA

```{r}
qda_out <- qda(CLASS_BINARY ~ Gender + HbA1c + Chol + TG + BMI,data = Data_training) # don't need CV = TRUE

Predicted.CLASS_qda  <-  predict(qda_out, data.frame(Data_training))$class

table(Data_training$CLASS_BINARY, Predicted.CLASS_qda)

# Prediction Correct Classification Rate
qda_ccr <- round(mean(Data_testing$CLASS_BINARY == Predicted.CLASS_qda), 3)
qda_ccr

round(mean(Predicted.CLASS_qda == "1"), 3)

round(mean(Data_testing$CLASS_BINARY == "1"), 3)



```

-   logistic classication

```{r}
lreg_fit <-  glm(CLASS_BINARY ~ Gender + HbA1c + Chol + TG + BMI, 
    family = "binomial", data = Data_training)
summary(lreg_fit)

lreg_fit2 <-  glm(CLASS_BINARY ~ Gender + HbA1c + Chol + TG + BMI, 
    family = "binomial", data = Data_training2)
summary(lreg_fit)



Predicted_probability <- predict(lreg_fit, data.frame(Data_testing),
                                 type = "response")
Predicted_probability2 <- predict(lreg_fit2, data.frame(Data_testing2),
                                 type = "response")
Predicted_diabetic = Predicted_probability > 0.5
Predicted_diabetic2 = Predicted_probability2 > 0.5

table(Data_testing$CLASS_BINARY, Predicted_diabetic)

table(Data_testing$CLASS_BINARY, Predicted_diabetic2)

# Prediction Correct Classification Rate
log_ccr <- mean(Data_testing$CLASS_BINARY == Predicted_diabetic)
log_ccr


log_ccr2 <- mean(Data_testing$CLASS_BINARY == Predicted_diabetic2)
log_ccr2




```

```{r}

Prob <- predict(lreg_fit, data.frame(Data_testing),
                                 type = "response")
threshold <- seq(0, 1, .01)
length(threshold)

head(threshold)

TPR <-  FPR <- err.rate <- rep(0, length(threshold))

for (i in seq_along(threshold)) {
Yhat <- rep(NA_character_, nrow(diabetes[Z,])) 
Yhat <-  ifelse(Prob >= threshold[[i]], "1", "0")

err.rate[i] <- mean(Yhat != diabetes[Z,]$CLASS_BINARY)
TPR[[i]] <- sum(Yhat == "yes" & diabetes[Z,]$CLASS_BINARY == "1") /
  sum(diabetes[Z,]$CLASS_BINARY == "1")
FPR[[i]] <- sum(Yhat == "1" & diabetes[Z,]$CLASS_BINARY == "0") /
  sum(diabetes[Z,]$CLASS_BINARY == "no")

}

ggplot(tibble(threshold, err.rate),
       aes(threshold, err.rate)) + 
  geom_point()

which.min(err.rate)

threshold[0]

min(err.rate)


```

# PCA

```{r}
library("ggplot2")
library("gridExtra")
#install.packages("ggbiplot")
library(ggbiplot)
library("corrplot")
library(factoextra)

Data.pca<-prcomp(diabetes[,c(2:10)], center = TRUE,scale. = TRUE)
summary(Data.pca)


#scree plot
pca<-Data.pca
eig.val = eig.val <- pca$sdev^2
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 50))


# to see the most contributing variables for each dimension
var=get_pca_var(pca)
corrplot(var$cos2, is.corr=FALSE)


#to see the most contributing variables for both dimension
fviz_cos2(pca, choice = "var", axes = 1:5)

#Graph of variables
fviz_pca_var(pca,
             col.var = "cos2",
             gradient.cols = c("red", "blue", "green"),
             repel = TRUE)

# plot using PC1 and PC2
ggbiplot(pca,ellipse=TRUE,choices=c(1,2),labels=rownames(diabetes), groups=diabetes$CLASS_BINARY)
# scale the samples
ggbiplot(pca,ellipse=TRUE,obs.scale = 2, var.scale = 4.5,
         labels=rownames(diabetes), groups=diabetes$CLASS_BINARY)
#remove the arrows altogether
ggbiplot(pca,ellipse=TRUE,obs.scale = 1, var.scale = 1,var.axes=FALSE,
         labels=rownames(diabetes), groups=diabetes$CLASS_BINARY)



library(ggbiplot)
library(ggplot2)

# Create the ggbiplot
plot <- ggbiplot(pca, ellipse = TRUE, obs.scale = 2, var.scale = 4.5, groups = diabetes$CLASS_BINARY)  +
  ggtitle("PCA of Diabetes Patients") +
  theme_minimal() +
  theme(legend.position = "bottom") + guides(colour = FALSE)

# Increase the size of the plot and save it
ggsave("pca_plot.png", plot, width = 10, height = 8)
```

# Apendix

```{r}
library(dplyr)

data_test <- diabetes[,c(2:11,14)]

# Define the function to perform ANOVA and tidy results
perform_anova <- function(data, group_var, ...) {
  # Select only numeric columns for ANOVA
  numeric_cols <- sapply(data, is.numeric)
  numeric_data <- data[, numeric_cols]
  
  # Perform ANOVA for each numeric column
  anova_results <- lapply(numeric_data, function(column) {
    aov_result <- aov(column ~ get(group_var), data = data)
    tidy(aov_result)
  })
  
  # Combine ANOVA results into a single dataframe
  anova_df <- do.call(rbind, anova_results)
  
  # Add column indicating the variable name
  anova_df$variable <- rownames(anova_df)
  
  return(anova_df)
}

# Apply the function to your data
data_anova <- diabetes  %>%
  do(perform_anova(data_test, "CLASS_BINARY"))

# Group by CLASS and calculate mean for each numeric variable
means_by_class <- data_test %>%
  group_by(CLASS_BINARY) %>%
  summarise_all(mean,na.rm = TRUE)

additional_values_df <- data.frame(
  CLASS = "pvalue",
  AGE = 6.405477e-55,
  Urea = 6.329202e-02,
  Cr = 4.956277e-01,
  HbA1c = 1.031427e-81,
  Chol = 6.955116e-07,
  TG = 4.386920e-08,
  HDL = 6.524445e-01,
  LDL = 7.348649e-01,
  VLDL = 3.810216e-03,
  BMI = 4.714323e-90
)

# Bind the data frames
means_by_class1 <- bind_rows(means_by_class, additional_values_df)

# View the updated means_by_class data frame
print(means_by_class1)

```
